{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name - Abhishek Shinde                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student ID - 1001754842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Importing All Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import random as rnd\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Positive And Negative Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Entered Path with the Path of your Dataset Directory for Both Positive and Negative Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = [x for x in os.listdir(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/train/pos/') if x.endswith(\".txt\")]\n",
    "negative_train = [x for x in os.listdir(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/train/neg/') if x.endswith(\".txt\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Contents of Positive and Negative Files and adding to Positive and Negative List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Entered Path with the Path of your Dataset Directory for Both Positive and Negative Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_Tr, negative_Tr = [], []\n",
    "for x in positive_train:\n",
    "    with open(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/train/pos/'+x, encoding=\"latin1\") as f:\n",
    "        positive_Tr.append(f.read())\n",
    "for y in negative_train:\n",
    "    with open(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/train/neg/'+y, encoding=\"latin1\") as f:\n",
    "        negative_Tr.append(f.read())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Fold Cross Validation By Splitting the Train Set into Train and Development Set Randomly and using Each Generated Development Set for Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train into Train And Development Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dev_p,dev_n = [],[]\n",
    "train_p,train_n = [],[]\n",
    "for i in range(0,5):\n",
    "        r=rnd.randint(0,100) \n",
    "        train_pos,dev_pos = train_test_split(positive_Tr,test_size = 0.2,random_state = r)\n",
    "        train_neg,dev_neg = train_test_split(negative_Tr,test_size = 0.2,random_state = r)\n",
    "        dev_p.append(dev_pos)\n",
    "        dev_n.append(dev_neg)\n",
    "        train_p.append(train_pos)\n",
    "        train_n.append(train_neg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Cleaning\n",
    "def TextClean(data):\n",
    "    txt = []\n",
    "    for T in data:\n",
    "        T = re.sub(r'@[A-Za-z0-9_]+','',T)\n",
    "        T = re.sub(r\"http\\S+\", \"\", T)\n",
    "        T = T.replace('<br />', '')\n",
    "        T = T.replace(\"\\'\",\"\")\n",
    "        T = T.replace(\"?'\",\"\")\n",
    "        T = T.replace(\"*\", \"\")\n",
    "        T = T.replace(\"/\", \"\")\n",
    "        T = T.replace(\"\\ \", \"\")\n",
    "        T = T.replace(\".\", \"\")\n",
    "        T = T.replace(\"(\", \"\")\n",
    "        T = T.replace(\")\", \"\")\n",
    "        T = T.replace(\":\", \"\")\n",
    "        T = T.replace('\"', \"\")\n",
    "        T = T.replace(\",\", \"\")\n",
    "        T = T.replace(\"!\", \"\")\n",
    "        T = T.replace(\"'\", \"\")\n",
    "        T = T.replace(\"&\", \"\")\n",
    "        T = re.sub(r\"[0-9]*\", \"\", T)\n",
    "        T = re.sub(r\"(”|“|-|\\+|`|#|,|;|\\|/|\\\\|)*\",\"\", T)\n",
    "        T = re.sub(r\"&amp\",\"\", T)\n",
    "        T = T.lower()\n",
    "        txt.append(T)\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Special Characters\n",
    "def Remove_SC(text):\n",
    "    alphabet = []\n",
    "    alpha = 'a'\n",
    "    for i in range(0, 26): \n",
    "        alphabet.append(alpha) \n",
    "        alpha = chr(ord(alpha) + 1)\n",
    "    l = []\n",
    "    for i in text:\n",
    "        txt = []\n",
    "        t = i.split(' ')\n",
    "        for j in t:\n",
    "            m = j\n",
    "            for k in m:\n",
    "                if k not in alphabet:\n",
    "                    m = m.replace(k, '')\n",
    "            if m != '':\n",
    "                txt.append(m)\n",
    "        #l.append(txt)\n",
    "        s = ''\n",
    "        for j in txt:\n",
    "            s = s + j + ' '\n",
    "        l.append(s)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THE TRAINING SET BY REMOVING THE PUNCTUATIONS, NUMBERS AND SPECIAL CHARACTERS\n",
    "Execution Of Code Block takes 25-30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution Takes 30 Seconds\n",
    "#Cleaning Training Dataset\n",
    "clean_train_pos = TextClean(train_pos)\n",
    "clean_train_neg = TextClean(train_neg)\n",
    "\n",
    "#Removing Special Characters\n",
    "clean_train_pos= Remove_SC(clean_train_pos)\n",
    "clean_train_neg= Remove_SC(clean_train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Combine Positive And Negative Sets\n",
    "def Combine(a,b):\n",
    "    combine = a + b\n",
    "    return combine\n",
    "\n",
    "#Combining Both Positive and Negative Train Values\n",
    "combine_clean_train= Combine(clean_train_pos,clean_train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization Function\n",
    "def Tokenize_Words(dataset):\n",
    "    stemset = []\n",
    "    for i in range(0,len(dataset)):\n",
    "        words =word_tokenize(dataset[i])\n",
    "        for w in words:\n",
    "            stemset.append(w)\n",
    "    return stemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the Entire Training Data To Easily Calculate the Probability of Vocabulary Words\n",
    "Execution Takes 22-30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution takes 30 seconds\n",
    "#Tokenizing The Train Set\n",
    "token_train = Tokenize_Words(combine_clean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing StopWords\n",
    "def removeStopwords(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    x = [w for w in dataset if not w in stop_words]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stopwords from Train Set\n",
    "stop_train = removeStopwords(token_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Vocabulary List \n",
    "Which consists of Words occuring more than 5 Times in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Vocabulary List of Words which appear more than 5 Times in the entire document\n",
    "import collections\n",
    "\n",
    "occurence_list = collections.Counter(stop_train)\n",
    "unique_occurence = dict(occurence_list)\n",
    "vocabulary_list = []\n",
    "for val in unique_occurence:\n",
    "    if(unique_occurence[val]>= 5):\n",
    "        vocabulary_list.append(val)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function To Create A vocabulary List for Every Train Set. But Since my Laptop is not Powerful Enough to run so many \n",
    "#executions at the same time, i am running vocabulay set building for one train set\n",
    "\n",
    "\n",
    "# def Vocab(settrain_p,settrain_n):\n",
    "#     #Cleaning Training Dataset\n",
    "#     start = time.time()\n",
    "#     clean_train_pos = TextClean(settrain_p)\n",
    "#     clean_train_neg = TextClean(settrain_n)\n",
    "\n",
    "#     #Removing Special Characters\n",
    "#     clean_train_pos= Remove_SC(clean_train_pos)\n",
    "#     clean_train_neg= Remove_SC(clean_train_neg)\n",
    "\n",
    "#     #Combining Both Positive and Negative Train Values\n",
    "#     combine_clean_train= Combine(clean_train_pos,clean_train_neg)\n",
    "#     #Tokenizing The Train Set\n",
    "#     token_train = Tokenize_Words(combine_clean_train)\n",
    "#     #Remove Stopwords from Train Set\n",
    "#     stop_train = removeStopwords(token_train)\n",
    "\n",
    "#     occurence_list = collections.Counter(stop_train)\n",
    "#     unique_occurence = dict(occurence_list)\n",
    "#     vocabulary_list = []\n",
    "#     for val in unique_occurence:\n",
    "#         if(unique_occurence[val]>= 5):\n",
    "#             vocabulary_list.append(val)\n",
    "    \n",
    "#     return vocabulary_list\n",
    "\n",
    "# for i in range(0,len(train_p)):\n",
    "#     vocabulary_list = Vocab(train_p[i],train_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Occurance of vocabulary list words in all docs of train set\n",
    "#i.e. how many documents does one word of the Vocabulary List Occur in\n",
    "#Execution Time - 13 minutes\n",
    "def Count_AllDocs(vocablist,dataset):\n",
    "    no_of_docs = 0\n",
    "    probability={}\n",
    "    for t in range(0,len(vocablist)):\n",
    "        temp = vocablist[t]\n",
    "        for i in range(0,len(dataset)):\n",
    "            if (dataset[i].count(temp) > 0):\n",
    "                no_of_docs = no_of_docs + 1\n",
    "        probability.update({temp:no_of_docs})\n",
    "        no_of_docs = 0\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATING THE OCCURRENCE OF EVERY WORD IN THE VOCABULARY LIST WITHIN THE ENTIRE TRAINING SET. ENTIRE TRAINING SET INCLUDES BOTH POSITIVE AND NEGATIVE REVIEWS\n",
    "\n",
    "This Dictionary Building Takes 15 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768.8560998439789\n"
     ]
    }
   ],
   "source": [
    "#Takes 15 minutes to execute\n",
    "prob_occ_train = Count_AllDocs(vocabulary_list,combine_clean_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Calculate: (Without Smoothing)  \n",
    "#C(1) P(\"word\") - num of documents containing ‘word’ / num of all documents\n",
    "def Probability_AllDocs(vocab_list,dataset):\n",
    "    total_documents = len(dataset)\n",
    "    probability = {}\n",
    "    for i in vocab_list:\n",
    "        prob =  vocab_list[i]/ total_documents \n",
    "        probability.update({i:prob})\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Probability of Every Word In Vocabulary List in The Entire Training Set\n",
    "P_word_alldocs = Probability_AllDocs(prob_occ_train,combine_clean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"With Smoothing\"\"\"\n",
    "#C(1) P(\"word\") - num of documents containing ‘word’ / num of all documents\n",
    "def Smooth_Probability_AllDocs(vocab_list,dataset):\n",
    "    total_documents = len(dataset)\n",
    "    probability = {}\n",
    "    for i in vocab_list:\n",
    "        prob =  (vocab_list[i]+1)/(total_documents+2)\n",
    "        probability.update({i:prob})\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Smoothing to Probability of All Words in Document\n",
    "sm_P_word_alldocs = Smooth_Probability_AllDocs(prob_occ_train,combine_clean_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Probability of Every Word IN the Vocabulary List with Either Positive Or Negative Set of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Occurance of vocabulary list words in POSITIVE or NEGATIVE docs of train set\n",
    "def Prob_WordsInPosOrNeg(vocablist,dataset):\n",
    "    count = 0\n",
    "    prob={}\n",
    "    for t in range(0,len(vocablist)):\n",
    "        temp = vocablist[t]\n",
    "        for i in range(0,len(dataset)):\n",
    "            if (dataset[i].count(temp) > 1):\n",
    "                count = count + 1\n",
    "        prob.update({temp:count})\n",
    "        count = 0\n",
    "    return prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.37950563430786\n"
     ]
    }
   ],
   "source": [
    "#Occurrence of Vocabulary Words in Positive Reviews\n",
    "#Takes 7-10 minutes to execute\n",
    "occ_prob_positive = Prob_WordsInPosOrNeg(vocabulary_list,clean_train_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378.90028142929077\n"
     ]
    }
   ],
   "source": [
    "#Occurrence of Words in Negative Reviews\n",
    "#Takes 7-10 minutes to execute\n",
    "occ_prob_negative = Prob_WordsInPosOrNeg(vocabulary_list,clean_train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONDITIONAL PROBABILITY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditional probability based on the sentiment\n",
    "#P[“the” | Positive]  =  of positive documents containing “the” / num of all positive review documents\n",
    "#P[“the” | Negative]  =  of negative documents containing “the” / num of all negative review documents\n",
    "\n",
    "def Conditional_Prob(vocab_list,dataset):\n",
    "    total_documents = len(dataset)\n",
    "    probability = {}\n",
    "    for i in prob_occ_train:\n",
    "        probability.update({i:vocab_list[i]/total_documents})\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONDITIONAL PROBABILITY OF WORDS OF VOCABULARY LIST, IN POSITIVE AND NEGATIVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditinoal Probability of Vocabulary Words given Positive \n",
    "cond_prob_train_pos = Conditional_Prob(occ_prob_positive,clean_train_pos)\n",
    "\n",
    "\n",
    "#Conditinoal Probability of Vocabulary Words given Negative\n",
    "cond_prob_train_neg = Conditional_Prob(occ_prob_negative,clean_train_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth_Conditional_Porb\n",
    "def Smooth_Conditional_Prob(vocab_list,dataset):\n",
    "    total_documents = len(dataset)\n",
    "    probability = {}\n",
    "    for i in prob_occ_train:\n",
    "        prob = (vocab_list[i]+1)/(total_documents+2)\n",
    "        probability.update({i:prob})\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditinoal Probability of Vocabulary Words given Positive \n",
    "sm_cond_prob_train_pos = Smooth_Conditional_Prob(occ_prob_positive,clean_train_pos)\n",
    "\n",
    "\n",
    "#Conditinoal Probability of Vocabulary Words given Negative\n",
    "sm_cond_prob_train_neg = Smooth_Conditional_Prob(occ_prob_negative,clean_train_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR NAIVE BAYES CLASSIFIER WITHOUT SMOOTHING TO PREDICT REVIEW SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBClassifier(x_dev,y_dev):\n",
    "    #Execution Takes 20-30 seconds\n",
    "    #Cleaning Development Set Dataset\n",
    "    clean_x_dev = TextClean(x_dev)\n",
    "    #Removing Special Characters\n",
    "    clean_x_dev= Remove_SC(clean_x_dev)\n",
    "    #Execution Takes 30 - 40 seconds\n",
    "    #Tokenization of Dev Set\n",
    "    y_pred=[]\n",
    "    stem_x_dev=[]\n",
    "    for i in range(0,len(clean_x_dev)):\n",
    "        stem_x_dev = Tokenize_Words(clean_x_dev[i].split(\" \"))\n",
    "        stop_x_dev = removeStopwords(stem_x_dev)\n",
    "        \n",
    "    #Building Vocabulary List of Words in Review Document\n",
    "        occurence_list = collections.Counter(stop_x_dev)\n",
    "        unique_occurence = dict(occurence_list)\n",
    "        vocabulary_list_dev = []\n",
    "        for val in unique_occurence:\n",
    "            if(unique_occurence[val]>= 0):\n",
    "                vocabulary_list_dev.append(val)\n",
    "        \n",
    "        \n",
    "        posprob=[]\n",
    "        for i in vocabulary_list_dev:\n",
    "            if(cond_prob_train_pos.get(i,0) !=0):\n",
    "                posprob.append(cond_prob_train_pos[i])\n",
    "    \n",
    "        prob = 1\n",
    "        for i in posprob:\n",
    "            prob=prob*i\n",
    "    \n",
    "        negprob=[]\n",
    "        for i in vocabulary_list_dev:\n",
    "            if(cond_prob_train_neg.get(i,0) !=0):\n",
    "                negprob.append(cond_prob_train_neg[i])\n",
    "        nprob = 1\n",
    "        for i in negprob:\n",
    "            nprob=nprob*i\n",
    "        if(prob > nprob):\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NBC With Smoothing Added to Probabilities\"\"\"\n",
    "def NBClassifier_Smooth(x_dev,y_dev):\n",
    "    #Execution Takes 20-30 seconds\n",
    "    #Cleaning Development Set Dataset\n",
    "    clean_x_dev = TextClean(x_dev)\n",
    "    #Removing Special Characters\n",
    "    clean_x_dev= Remove_SC(clean_x_dev)\n",
    "    #Execution Takes 30 - 40 seconds\n",
    "    #Tokenization of Dev Set\n",
    "    y_pred=[]\n",
    "    stem_x_dev=[]\n",
    "    for i in range(0,len(clean_x_dev)):\n",
    "        stem_x_dev = Tokenize_Words(clean_x_dev[i].split(\" \"))\n",
    "        stop_x_dev = removeStopwords(stem_x_dev)\n",
    "        \n",
    "    #Building Vocabulary List of Words in Review Document\n",
    "        occurence_list = collections.Counter(stop_x_dev)\n",
    "        unique_occurence = dict(occurence_list)\n",
    "        vocabulary_list_dev = []\n",
    "        for val in unique_occurence:\n",
    "            if(unique_occurence[val]>= 0):\n",
    "                vocabulary_list_dev.append(val)\n",
    "        \n",
    "        \n",
    "        posprob=[]\n",
    "        for i in vocabulary_list_dev:\n",
    "            if(sm_cond_prob_train_pos.get(i,0) !=0):\n",
    "                posprob.append(sm_cond_prob_train_pos[i])\n",
    "    \n",
    "        prob = 1\n",
    "        for i in posprob:\n",
    "            prob=prob*i\n",
    "    \n",
    "        negprob=[]\n",
    "        for i in vocabulary_list_dev:\n",
    "            if(sm_cond_prob_train_neg.get(i,0) !=0):\n",
    "                negprob.append(sm_cond_prob_train_neg[i])\n",
    "        nprob = 1\n",
    "        for i in negprob:\n",
    "            nprob=nprob*i\n",
    "        if(prob > nprob):\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrame\n",
    "def MakeDF(a,b):\n",
    "    data_pos = {'Review':[i for i in a] , 'Sentiment':1}\n",
    "    data_pos = pd.DataFrame(data_pos,columns = ['Review','Sentiment'])\n",
    "    \n",
    "    data_neg = {'Review':[i for i in b] , 'Sentiment':0}\n",
    "    data_neg = pd.DataFrame(data_neg,columns = ['Review','Sentiment'])\n",
    "    dev_set = data_pos.append(data_neg)\n",
    "    x_dev=[]\n",
    "    for i in dev_set['Review']:\n",
    "        x_dev.append(i)\n",
    "    y_dev = []\n",
    "    for i in dev_set['Sentiment']:\n",
    "        y_dev.append(i)\n",
    "        \n",
    "    return x_dev,y_dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 sets of Development Set so we will Calculate the Accuracy for each of these sets Without Smoothing and calculate the average of their accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NBC without Smoothing For Development Set  1  :  47.12\n",
      "Accuracy of NBC without Smoothing For Development Set  2  :  48.28\n",
      "Accuracy of NBC without Smoothing For Development Set  3  :  48.0\n",
      "Accuracy of NBC without Smoothing For Development Set  4  :  47.78\n",
      "Accuracy of NBC without Smoothing For Development Set  5  :  51.239999999999995\n",
      "466.39870405197144\n"
     ]
    }
   ],
   "source": [
    "#Execution of all 5 Development Sets takes 10 minutes\n",
    "\n",
    "#Without Smoothing\n",
    "\n",
    "\n",
    "#List to keep track of all Accuracies for all Development Sets\n",
    "acc_list=[]\n",
    "# len(dev_p)\n",
    "for i in range(0,5):\n",
    "    x,y = MakeDF(dev_p[i],dev_n[i])\n",
    "    \n",
    "    y_pred = NBClassifier(x,y)\n",
    "    \n",
    "    acc = accuracy_score(y, y_pred)*100\n",
    "    \n",
    "    print(\"Accuracy of NBC without Smoothing For Development Set \",(i+1),\" : \", acc,\" %\")\n",
    "    acc_list.append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy of Development Set =  48.93599999999999 %\n"
     ]
    }
   ],
   "source": [
    "#Averaging the Accuracy of 5 Fold Cross Validation\n",
    "avg_acc = sum(acc_list)/len(acc_list)\n",
    "print(\"Average Accuracy of Development Set = \", avg_acc , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NBC with Smoothing For Development Set  1  :  77.3  %\n",
      "Accuracy of NBC with Smoothing For Development Set  2  :  76.58  %\n",
      "Accuracy of NBC with Smoothing For Development Set  3  :  77.14  %\n",
      "Accuracy of NBC with Smoothing For Development Set  4  :  76.84  %\n",
      "Accuracy of NBC with Smoothing For Development Set  5  :  75.66000000000001  %\n",
      "479.5948841571808\n"
     ]
    }
   ],
   "source": [
    "#Execution of all 5 Development Sets takes 10 minutes\n",
    "\n",
    "#With Smoothing \n",
    "\n",
    "\n",
    "#List to keep track of all Accuracies for all Development Sets\n",
    "accs_list=[]\n",
    "# len(dev_p)\n",
    "for i in range(0,5):\n",
    "    x,y = MakeDF(dev_p[i],dev_n[i])\n",
    "    \n",
    "    y_pred = NBClassifier_Smooth(x,y)\n",
    "    \n",
    "    acc = accuracy_score(y, y_pred)*100\n",
    "    \n",
    "    print(\"Accuracy of NBC with Smoothing For Development Set \",(i+1),\" : \", acc,\" %\")\n",
    "    accs_list.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy of Development Set =  76.70400000000001 %\n"
     ]
    }
   ],
   "source": [
    "#Averaging the Accuracy of 5 Fold Cross Validation\n",
    "avg_acc = sum(accs_list)/len(accs_list)\n",
    "print(\"Average Accuracy of Development Set = \", avg_acc , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nP['positive'|'word'] = P['word'|'positive'] * P['positive']  /  P['word']\\n\\n\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Derive Top 10 words that predicts positive and negative class\n",
    "                P[Positive| word] \n",
    "                \n",
    "\"\"\"\n",
    "\"\"\" WITHOUT SMOOTHING\"\"\"\n",
    "#P['word']- for both positive and negative:-    P_word_alldocs\n",
    "\n",
    "#P['word'|'positive'] :-                        cond_prob_train_pos\n",
    "\n",
    "#P['positive'] : -                              0.5\n",
    "\n",
    "#P['word'|'negative'] :-                        cond_prob_train_neg\n",
    "\n",
    "#P['neg'] : -                                   0.5\n",
    "\n",
    "\"\"\"\n",
    "P['positive'|'word'] = P['word'|'positive'] * P['positive']  /  P['word']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Top 10 Words for Positive and Negative\n",
    "import operator\n",
    "def Derive(a,b):\n",
    "    \n",
    "    words_list={}\n",
    "    desc_dict={}\n",
    "    for (i,j) in zip(a,b):\n",
    "        temp1 = a[i]\n",
    "        temp2 = b[i]\n",
    "        prob = ( temp1 * 0.5 ) / temp2\n",
    "        words_list.update({i:prob})\n",
    "        \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATING PROBABILITY OF POSITIVE AND NEGATIVE GIVEN WORD. I.E. P['POSITIVE'|'WORD] AND P['NEGATIVE'|'WORD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Probability of Positive / Negative given all Words\n",
    "top_pos_words = Derive(cond_prob_train_pos,P_word_alldocs)\n",
    "top_neg_words = Derive(cond_prob_train_neg,P_word_alldocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Top 10 Positive and Negative Words with Highest Probability\n",
    "def Cal_Top_Ten(probabilities):\n",
    "    temp = sorted(probabilities.items(), key=lambda x: x[1],reverse=True)\n",
    "    ten_words = []\n",
    "    for i in range(0,10):\n",
    "        ten_words.append(temp[i][0])\n",
    "    return ten_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING THE TOP 10 WORDS THAT PREDICT POSITIVE AND NEGATIVE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 Positive AND Negative Words\n",
    "positive_topten = Cal_Top_Ten(top_pos_words)\n",
    "negative_topten = Cal_Top_Ten(top_neg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOP 10 POSITIVE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words that predicts positive : -  ['frwl', 'flavia', 'hiralal', 'thursby', 'bouzaglo', 'jermaine', 'tetsur', 'maetel', 'spanky', 'cybersix']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 words that predicts positive : - \",positive_topten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOP 10 NEGATIVE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words that predicts positive : -  ['calson', 'emran', 'bohlen', 'pooja', 'hallam', 'hyuck', 'munkar', 'mameha', 'huggaland', 'hugsy']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 words that predicts positive : - \",negative_topten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLYING NBC MODEL WITH SMOOTHING TO CALCULATE ACCURACY FOR OUR MAIN TEST SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Entered Path with the Path of your Dataset Directory for Both Positive and Negative Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Test Set Data\n",
    "positive_test = [x for x in os.listdir(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/test/pos/') if x.endswith(\".txt\")]\n",
    "negative_test = [x for x in os.listdir(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/test/neg/') if x.endswith(\".txt\")]\n",
    "test_pos, test_neg = [], []\n",
    "for x in positive_test:\n",
    "    with open(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/test/pos/'+x, encoding=\"latin1\") as f:\n",
    "        test_pos.append(f.read())\n",
    "for y in negative_test:\n",
    "    with open(r'F:/UTA Documents/UTA/Spring 20/Data Mining/Dataset Ass3/aclImdb/test/neg/'+y, encoding=\"latin1\") as f:\n",
    "        test_neg.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Test Set DataFrame\n",
    "x_test,y_test = MakeDF(test_pos,test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive-Bayes Classifier with Smoothing For Test Set :  75.81599999999999  %\n",
      "461.6448860168457\n"
     ]
    }
   ],
   "source": [
    "#Execution takes 8 to 10 minutess \n",
    "y_pred_sm = NBClassifier_Smooth(x_test,y_test)\n",
    "print(\"Accuracy of Naive-Bayes Classifier with Smoothing For Test Set : \", accuracy_score(y_test, y_pred_sm)*100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HENCE WE HAVE PREDICTED THE SENTIMENT OF OUR TEST SET USING THE TRAINING SET DATA WITH AN ACCURACY OF 76.40%.\n",
    "\n",
    "SMOOTHING DEFINITELY PLAYS AN IMPORTANT ROLE IN HELPING OVERCOME THE 0-PROBABILITY ISSUE WHICH WOULD ARISE DURING PROBABILITY CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
